{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se certifique que você está utilizando Python 3 e utilizando UTF-8\n",
    "print(sys.getdefaultencoding())\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Google Scholar Crawler\n",
    "# Para executar o script substitua o filename com o nome do arquivo que contém as URLs de busca\n",
    "#\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Próximas etapas: \n",
    "# - Começar a baixar os PDFs\n",
    "\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "last_url = ''\n",
    "last_soup = ''\n",
    "last_author = ''\n",
    "PROXY_LIST = []\n",
    "\n",
    "USER_AGENT_CHOICES = [\n",
    "    'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:23.0) Gecko/20100101 Firefox/23.0',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.62 Safari/537.36',\n",
    "    'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; WOW64; Trident/6.0)',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.146 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.146 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20140205 Firefox/24.0 Iceweasel/24.3.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2',\n",
    "]\n",
    "\n",
    "from lxml.html import fromstring\n",
    "import traceback\n",
    "\n",
    "def get_proxies():\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    headers = {'user-agent': random.choice(USER_AGENT_CHOICES) }\n",
    "    response = requests.get(url,  headers=headers)\n",
    "    parser = fromstring(response.text)\n",
    "    lista = []\n",
    "    for i in parser.xpath('//tbody/tr')[:10]:\n",
    "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "            lista.append(proxy)\n",
    "    print('Proxies obtidos: %d' % len(lista))\n",
    "    return lista\n",
    "\n",
    "def get_page(url):\n",
    "    global PROXY_LIST\n",
    "    last_soup = ''\n",
    "    headers = {'user-agent': random.choice(USER_AGENT_CHOICES) }\n",
    "    while len(PROXY_LIST) > 0:\n",
    "        try:\n",
    "            proxy = random.choice(PROXY_LIST)\n",
    "            response = requests.get(url, headers=headers, proxies={ \"http\": proxy, \"https\": proxy, },timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                print('%s (%s)' % (url,proxy))\n",
    "                if u'Ative o JavaScript' in response.text:\n",
    "                    print('Proxy Blocked 1')\n",
    "                    raise Exception('Blocked')\n",
    "                last_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                break\n",
    "            else:\n",
    "                print('Proxy Blocked 2')\n",
    "                raise Exception('Blocked')\n",
    "        except:\n",
    "            PROXY_LIST.remove(proxy)\n",
    "            print('Dead Proxy: %s - Left: %d' % (proxy, len(PROXY_LIST)) )\n",
    "            while len(PROXY_LIST) == 0:\n",
    "                time.sleep(60)\n",
    "                PROXY_LIST = get_proxies()\n",
    "            \n",
    "    return last_soup\n",
    "\n",
    "def find_artigos(soup):\n",
    "    try:\n",
    "        lista = soup.find('div', attrs={'id': 'gs_bdy'}).find('div', attrs={'id': 'gs_bdy_ccl'})\n",
    "        lista = lista.find('div',attrs={'id': 'gs_res_ccl'})\n",
    "        lista = lista.find('div',attrs={'id': 'gs_res_ccl_mid'})\n",
    "    except:\n",
    "        if soup:\n",
    "            print('erro:'+soup.text)\n",
    "        lista = None\n",
    "    return lista\n",
    "\n",
    "def parse_pagina(lista, artigos, origem):\n",
    "    incluidos = 0\n",
    "    for registro in lista.findAll('div',attrs={'class':'gs_r gs_or gs_scl'}):\n",
    "\n",
    "        tipo = registro.find('div',attrs={'class':'gs_or_ggsm'})\n",
    "        if tipo:\n",
    "            tipo = tipo.find('span').text\n",
    "        else:\n",
    "            tipo = 'HTML'\n",
    "            \n",
    "        artigo = { 'tipo': tipo, 'citado': 0 } # Contem a indicação se é HTML ou PDF\n",
    "        \n",
    "        item = registro.find('div',attrs={'class':'gs_ri'})\n",
    "        divs = item.findAll('div')\n",
    "        titulo = item.find('h3')\n",
    "        if titulo and titulo.find('a'):\n",
    "            artigo['url'] = titulo.find('a')['href']\n",
    "            artigo['titulo'] =  titulo.find('a').text\n",
    "        else:\n",
    "            artigo['titulo'] = 'Titulo não encontrado (%d)' % incluidos\n",
    "            artigo['url'] = last_url\n",
    "\n",
    "        detalhe = divs[0].text.split('- ')\n",
    "        autores = {}\n",
    "        for autor in detalhe[0].split(','):\n",
    "            nome = autor.strip()\n",
    "            autores[ nome ] = ''\n",
    "        \n",
    "        for alink in divs[0].findAll('a'):\n",
    "            nome = alink.text.split('- ')[0].strip()\n",
    "            autores[ nome ] = alink['href'].split('&')[0]\n",
    "    \n",
    "        #for autor in sorted(autores.keys()):\n",
    "        #    print('Autor :%s' % autor)\n",
    "        #    print('Link  :%s' % autores[autor])\n",
    "               \n",
    "        artigo['autores'] = autores\n",
    "\n",
    "        year = ''\n",
    "        journal = ''\n",
    "        if len(detalhe) > 1:\n",
    "            journal = detalhe[1].strip()\n",
    "            if len(journal.split(',')) > 1:\n",
    "                year = journal.rsplit(',',1)[1].strip()\n",
    "                journal = journal.split(',')[0].strip()\n",
    "            else:\n",
    "                if len(journal) == 4:\n",
    "                    year = journal\n",
    "                    journal = ''\n",
    "                \n",
    "        artigo['journal'] = journal\n",
    "        artigo['year'] = year\n",
    "        artigo['origem'] = origem\n",
    "\n",
    "        try:\n",
    "            item = divs[2]\n",
    "            for celula in item.findAll('a'):\n",
    "                if celula.text[0:3].upper() == 'CIT' and len(celula.text.split(' ')) > 1:   # Cited by ou citado por\n",
    "                    artigo['citado'] = celula.text.split(' ')[2]\n",
    "        except:\n",
    "            artigo['citado'] = 0\n",
    "        \n",
    "        artigos.append( artigo )\n",
    "        incluidos += 1\n",
    "    return incluidos\n",
    "\n",
    "# Monta a lista de URLs que serão utilizadas\n",
    "#\n",
    "# exemplos de url que busca um artigo determinado:\n",
    "# https://scholar.google.com.br/scholar?hl=pt-BR&lr=lang_pt&as_sdt=0,5&q=label%3A+digital+humanities\n",
    "#\n",
    "# exemplos de url que cita um determinado artigo e que tem o retorno específico em português:\n",
    "# https://scholar.google.com/scholar?cites=537519748848001123&as_sdt=2005&sciodt=0,5&lr=lang_pt&hl=pt-BR'\n",
    "# https://scholar.google.com.br/scholar?lr=lang_pt&hl=pt-BR&as_sdt=0,5&sciodt=0,5&cites=17950367634186489359,349589428917357430,1179427091885643356\n",
    "\n",
    "urls = []       \n",
    "filename = 'cesta2.txt'\n",
    "if not os.path.isfile(filename):\n",
    "    raise Exception('Arquivo %s não encontrado' % filename)\n",
    "cesta = open(filename, 'r')\n",
    "for linha in cesta:\n",
    "    urls.append( linha.strip() )\n",
    "cesta.close()\n",
    "\n",
    "# Monta lista de Proxies\n",
    "PROXY_LIST = get_proxies()\n",
    "if len(PROXY_LIST) == 0:\n",
    "    raise Exception('No proxies found')\n",
    "\n",
    "# realizar o scraping de cada URL e armazenar os artigos encontrados na lista artigos\n",
    "# a rotina armazena também quantos registros foram obtidos em cada consulta\n",
    "url_index = 1\n",
    "artigos = []\n",
    "resultado_por_url = []\n",
    "for url_base in urls:\n",
    "    total = 0\n",
    "    incluidos = 0 \n",
    "    while True:\n",
    "        last_url = url_base+'&as_vis=1&start=%d' % total\n",
    "        soup = get_page( last_url )\n",
    "        lista = find_artigos(soup)\n",
    "        if lista:\n",
    "            incluidos = parse_pagina(lista, artigos, url_index)\n",
    "            total += incluidos\n",
    "            if incluidos < 10:\n",
    "                break\n",
    "    # necessário para que não hajam muitas requisições instantâneas para que o Google não bloqueie mais\n",
    "    # rapidamente as consultas\n",
    "    time.sleep(10)\n",
    "    url_index += 1\n",
    "    resultado_por_url.append(total)\n",
    "print('Fim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(artigos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero = 0\n",
    "for artigo in artigos:\n",
    "    titulo = artigo['titulo']\n",
    "    print(numero, titulo, artigo['autores'])\n",
    "    numero += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo as URLs que tiveram resultado nulo\n",
    "# Essa rotina só é necessária para otimizar o processo, fazendo com que a rotina realize menos consultas ao GS\n",
    "import csv\n",
    "index = 0\n",
    "gravados = 0\n",
    "with open('cesta2.txt', 'w') as arquivo:\n",
    "    for url in urls:\n",
    "        if len(resultado_por_url) <= index or resultado_por_url[ index ] > 0:\n",
    "            arquivo.write(url+'\\n')\n",
    "            gravados += 1\n",
    "        index += 1\n",
    "    arquivo.close()\n",
    "print('Lidos %d' % index)\n",
    "print('Gravados %d' % gravados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grava \n",
    "import csv\n",
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['titulo','autores','ano','citacoes','tipo','url','origem'])\n",
    "    for artigo in artigos:\n",
    "        if 'citado' in artigo:\n",
    "            citacoes = artigo['citado']\n",
    "        else:\n",
    "            citacoes = 0\n",
    "        writer.writerow([artigo['titulo'],artigo['autores'],artigo['year'],citacoes,artigo['tipo'],artigo['url'],artigo['origem']])\n",
    "print('arquivo gravado com sucesso!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cesta Final contém as referências das URLs pesquisadas\n",
    "# Necessário para que se possa descobrir qual a busca que recuperou o arquivo (Campo Origem)\n",
    "import csv\n",
    "with open('cestafinal.txt', 'w') as arquivo:\n",
    "    for url in urls:\n",
    "        arquivo.write(url+'\\n')\n",
    "    arquivo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retira os registros duplicados\n",
    "# Corrige também os registros que não tiveram o campo 'citado' registrado nos casos em que ele não foi encontrado\n",
    "# Remove artigos cuja origem estava errada\n",
    "\n",
    "base = set()\n",
    "dedup = []\n",
    "for d in artigos:\n",
    "    if d['origem'] != 56:\n",
    "        t = tuple(d['titulo'].strip())\n",
    "        if t not in base:\n",
    "            base.add(t)\n",
    "            dedup.append(d)\n",
    "\n",
    "print(len(dedup))\n",
    "\n",
    "import csv\n",
    "with open('dedup.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=';', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    writer.writerow(['titulo','autores','ano','citacoes','tipo','url','origem'])\n",
    "    for artigo in dedup:\n",
    "        if 'citado' in artigo:\n",
    "            citacoes = artigo['citado']\n",
    "        else:\n",
    "            citacoes = 0\n",
    "        writer.writerow([artigo['titulo'],artigo['autores'],artigo['year'],citacoes,artigo['tipo'],artigo['url'],artigo['origem']])\n",
    "print('arquivo gravado com sucesso!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Relação de autores únicos\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "totais = Counter()\n",
    "autores = {}\n",
    "for reg in dedup:\n",
    "    for autor, link in reg['autores'].items():\n",
    "        autor_normalizado = autor.replace('…','').strip().upper()\n",
    "        if autor_normalizado in autores:\n",
    "            if autores[autor_normalizado] == '' and link != '':\n",
    "                autores[autor_normalizado] = link\n",
    "            if autores[autor_normalizado] != link:\n",
    "                print('different URL: %s %s %s' % (autor_normalizado,autores[autor_normalizado],link))\n",
    "        else:\n",
    "            autores[autor_normalizado] = link\n",
    "        totais[autor_normalizado] += 1\n",
    "            \n",
    "print(len(autores))\n",
    "freqs = 0\n",
    "for top in totais.most_common():\n",
    "    if top[1] > 1:\n",
    "        print(top)\n",
    "        freqs +=1 \n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artigos da Cesta com mais citações\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "totais = Counter()\n",
    "for reg in dedup:\n",
    "    totais[ reg['origem'] ] += 1\n",
    "\n",
    "print(len(totais))\n",
    "for top in totais.most_common():\n",
    "    index = top[0] - 1\n",
    "    if index >= len(urls):\n",
    "        print('Indice fora da faixa! %d' % index )\n",
    "    else:\n",
    "        print(urls[ index ], top[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relação de autores com mais co-autoria\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "totais = Counter()\n",
    "coautorias = {}\n",
    "for reg in dedup:\n",
    "    if len(reg['autores']) > 1:\n",
    "        coautoria = ''\n",
    "        for autor, link in reg['autores'].items():\n",
    "            coautoria = coautoria + autor.replace('…','').strip().upper()+','\n",
    "        coautoria = coautoria[:-1]\n",
    "        totais[coautoria] += 1\n",
    "\n",
    "print(len(totais))\n",
    "for top in totais.most_common():\n",
    "    print(top)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
